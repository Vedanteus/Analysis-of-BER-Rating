# -*- coding: utf-8 -*-
"""DataMining_CAONE_Final.ipynb

Automatically generated by Colaboratory.

Original file is located at
    https://colab.research.google.com/drive/18MFNt_GgYd5UENV30WzzcW2uOrOzAYG-
"""

#############################################################
####################Group Members############################
#############################################################
#### John Munonye Uzuegbu   - 10632668      ################
#### Vijay Sivakumar        - 20002291      ################
#### Vedant Tomer           - 20015122      ################
#### Muhammet Furkan Yavuz  - 20014641      ################
#### Shahzad Saeed          - 10627692      ################
#############################################################

#Data Preparation for (DataMining CA_One)
import numpy as np
import pandas as pd
from pandas import read_csv, get_dummies, Series,DataFrame
from sklearn.preprocessing import StandardScaler
from sklearn.model_selection import GridSearchCV
from sklearn.model_selection import train_test_split

# imblearn library can be installed using pip install imblearn
from imblearn.over_sampling import SMOTE
from sklearn.ensemble import RandomForestClassifier, AdaBoostClassifier
from imblearn.pipeline import Pipeline

# Importing dataset and examining it
dataset = pd.read_csv("/content/drive/MyDrive/Data Mining CA_One/BERPublicsearch 300001_v1.csv")
# to make sure you can see all the columns in output window
pd.set_option('display.max_columns', None)
dataset.info()

#Heatmap
# Importing necessary
from plotly import graph_objs, figure_factory

# Selecting the first 30 columns of the dataset
data1=dataset.iloc[:,:30]
correlation  = data1.corr()

#print (correlation)
f = figure_factory.create_annotated_heatmap(correlation.values,list(correlation.columns),list(correlation.columns),correlation.round(2).values,showscale=True)
f.show()

#Data Cleaning
#Dropping features based on blanks, similarities and characteristics
# Features that relates to building characteristics and heating system efficiency.
# Inclusive of columns with more than 100k blank spaces.
dataset_clean1 = dataset.drop([
    'CountyName', 'Year_of_Construction', 'TypeofRating', 'gsdHSSupplHeatFraction',
    'gsdHSSupplSystemEff', 'DistLossFactor', 'CHPUnitHeatFraction', 'CHPSystemType',
    'CHPElecEff', 'CHPHeatEff', 'CHPFuelType', 'SupplHSFuelTypeID', 'gsdSHRenewableResources',
    'gsdWHRenewableResources', 'SolarHeatFraction',

# Features with energy assessment details and boiler specifications. They are mostly comments.
    'DateOfAssessment', 'FirstEnergyTypeId', 'FirstEnerProdComment', 'FirstEnerConsumedComment',
    'SecondEnergyTypeId', 'SecondEnerProdComment', 'SecondEnerConsumedComment', 'ThirdEnergyTypeId',
    'ThirdEnerProdComment', 'ThirdEnerConsumedComment', 'FirstBoilerFuelType', 'FirstHeatGenPlantEff',
    'FirstPercentageHeat', 'SecondBoilerFuelType', 'SecondHeatGenPlantEff', 'SecondPercentageHeat',
    'ThirdBoilerFuelType', 'ThirdHeatGenPlantEff', 'ThirdPercentageHeat', 'SolarSpaceHeatingSystem',

#Feaures regarding building wall descriptions and thermal characteristics, which are mostly blanks spaces.
    'TotalPrimaryEnergyFact', 'TotalCO2Emissions', 'FirstWallDescription', 'FirstWallIsSemiExposed',
    'FirstWallTypeId', 'FirstWallAgeBandId', 'SecondWallType_Description', 'SecondWallDescription',
    'ThirdWallType_Description', 'ThirdWallDescription', 'SecondWallAgeBandId', 'SecondWallTypeId',
    'ThirdWallType_Description', 'ThirdWallDescription', 'ThirdWallArea', 'ThirdWallUValue',
    'ThirdWallIsSemiExposed', 'ThirdWallAgeBandId', 'ThirdWallTypeId', 'SA_Code', 'RER', 'RenewEPnren',
    'prob_smarea_error_0corr', 'prob_smarea_error_100corr', 'RenewEPren', 'CPC', 'EPC', 'ApertureArea',
    'ZeroLossCollectorEff', 'CollectorHeatLossCoEff', 'AnnualSolarRadiation', 'OvershadingFactor',
    'SolarStorageVolume', 'VolumeOfPreHeatStore', 'SecondWallArea', 'SecondWallUValue',
    'SecondWallIsSemiExposed',

#These features relates to electricity consumption, delivered energy, and supplementary water.
#They are more than 100k blank spaces and mostly insignificant to the dataset.
    'ElectricityConsumption', 'TotalDeliveredEnergy', 'DeliveredEnergySupplementaryWater',
    'CO2SupplementaryWater', 'SWHPumpSolarPowered', 'ChargingBasisHeatConsumed',

#Dropping BerRating due to high correlation to Y (predicted field).
'BerRating',], axis = 1)

#Mapping and Converting Object Features to Numeric
#Guided Ranking Target Mapping

dataset_clean1['DwellingTypeDescr'] = dataset_clean1['DwellingTypeDescr'].map({'Semi-detached house':1,
  'End of terrace house':1,'House':1,'Detached house':1,'Mid-terrace house':1,'Ground-floor apartment':2,
  'Top-floor apartment':2,'Apartment':2,'Mid-floor apartment':2,'Basement Dwelling':0,'Maisonette':3}) #encoding
dataset_clean1['EnergyRating'] = dataset_clean1['EnergyRating'].map({'C2':0, 'B2':1, 'D1':0, 'C1':0,
  'C3':0, 'G ':0, 'D2':0, 'A3':1, 'F ':0, 'E1':0, 'E2':0,'B3':1, 'B1':1, 'A2':1, 'A1':1}) #encoding
dataset_clean1['MainSpaceHeatingFuel'] = dataset_clean1['MainSpaceHeatingFuel'].map({
    'Heating Oil                   ':1, 'Mains Gas                     ':1,
       'Electricity                   ':1, 'Bulk LPG (propane or butane)  ':1,
       'Solid Multi-Fuel              ':2, 'House Coal                    ':2,
       'Wood Pellets (bulk supply for ':2, 'Wood Logs                     ':2,
       'Bottled LPG                   ':1, 'Manufactured Smokeless Fuel   ':2,
       'Sod Peat                      ':3, 'Anthracite                    ':3,
       'Wood Chips                    ':3, 'Peat Briquettes               ':3,
       'Wood Pellets (in bags for seco':3, 'Electricity - Standard Domesti':1,}) #encoding
dataset_clean1['MainWaterHeatingFuel'] = dataset_clean1['MainWaterHeatingFuel'].map({
    'Heating Oil                   ':1, 'Mains Gas                     ':1,
       'Electricity                   ':1, 'Bulk LPG (propane or butane)  ':1,
       'Solid Multi-Fuel              ':2, 'House Coal                    ':2,
       'Bottled LPG                   ':3, 'Wood Pellets (bulk supply for ':3,
       'Wood Logs                     ':3, 'Sod Peat                      ':3,
       'Peat Briquettes               ':3, 'Anthracite                    ':3,
       'Manufactured Smokeless Fuel   ':3, 'Wood Chips                    ':3,
       'Wood Pellets (in bags for seco':3, 'Bioethanol from renewable sour':2,
       'Biodiesel from renewable sourc':2,}) #encoding
dataset_clean1['MultiDwellingMPRN'] = dataset_clean1['MultiDwellingMPRN'].map({'YES':1,'NO':0}) #encoding
dataset_clean1['DraftLobby'] = dataset_clean1['DraftLobby'].map({'YES':1,'NO':0}) #encoding
dataset_clean1['StructureType'] = dataset_clean1['StructureType'].map({
      'Timber or Steel Frame         ':2, 'Masonry                       ':1,
       'Insulated Conctete Form       ':0, 'Please select                 ':0}) #encoding
dataset_clean1['SuspendedWoodenFloor'] = dataset_clean1['SuspendedWoodenFloor'].map({
       'No                            ':0, 'Yes (Sealed)                  ':3,
       'Yes (Unsealed)                ':2}) #encoding
dataset_clean1['InsulationType'] = dataset_clean1['InsulationType'].map({
       'Loose Jacket                  ':1, 'Factory Insulated             ':2,
       'None                          ':0}) #encoding
dataset_clean1['VentilationMethod'] = dataset_clean1['VentilationMethod'].map({
       'Natural vent.':0, 'Bal.whole mech.vent heat recvr':3,
       'Pos input vent.- loft':1, 'Whole house extract vent.':1,
       'Bal.whole mech.vent no heat re':2, 'Pos input vent.- outside':2}) #encoding
dataset_clean1['PermeabilityTest'] = dataset_clean1['PermeabilityTest'].map({'YES':1,'NO':0}) #encoding
dataset_clean1['CHBoilerThermostatControlled'] = dataset_clean1['CHBoilerThermostatControlled'].map({'YES':1,'NO ':0}) #encoding
dataset_clean1['OBBoilerThermostatControlled'] = dataset_clean1['OBBoilerThermostatControlled'].map({'YES':1,'NO ':0}) #encoding
dataset_clean1['OBPumpInsideDwelling'] = dataset_clean1['OBPumpInsideDwelling'].map({'YES':1,'NO ':0}) #encoding
dataset_clean1['WarmAirHeatingSystem'] = dataset_clean1['WarmAirHeatingSystem'].map({'YES':1,'NO ':0}) #encoding
dataset_clean1['UndergroundHeating'] = dataset_clean1['UndergroundHeating'].map({'YES':1,'NO ':0}) #encoding
dataset_clean1['StorageLosses'] = dataset_clean1['StorageLosses'].map({'YES':1,'NO ':0}) #encoding
dataset_clean1['ManuLossFactorAvail'] = dataset_clean1['ManuLossFactorAvail'].map({'YES':1,'NO ':0}) #encoding
dataset_clean1['SolarHotWaterHeating'] = dataset_clean1['SolarHotWaterHeating'].map({'YES':1,'NO ':0}) #encoding
dataset_clean1['ElecImmersionInSummer'] = dataset_clean1['ElecImmersionInSummer'].map({'YES':1,'NO ':0}) #encoding

dataset_clean1['CombiBoiler'] = dataset_clean1['CombiBoiler'].map({
      'None                          ':0, 'Instantaneous, without keep-ho':2,
       'Storage combi boiler store vol':1, 'Instantaneous, with keep-hot f':3}) #encoding

dataset_clean1['KeepHotFacility']=dataset_clean1['KeepHotFacility'].map({'None                          ':0,
      'keep-hot facility, not control':1,'keep-hot facility, controlled ':2,})#encoding
dataset_clean1['PrimaryCircuitLoss']=dataset_clean1['PrimaryCircuitLoss'].map({'Boiler with insulated primary ':0,
      'Boiler with uninsulated primar':1,
      'Community heating             ':2,'Combi boiler                  ':3,
      'Boiler and thermal store withi':4,'None                          ':5,
      'Separate boiler and thermal st':6,'CPSU (including electric CPSU)':7,
      'Electric immersion heater     ':8,})#encoding
dataset_clean1['CylinderStat']=dataset_clean1['CylinderStat'].map({'NO ':0,'YES':1,})#encoding
dataset_clean1['CombinedCylinder']=dataset_clean1['CombinedCylinder'].map({'NO ':0,'YES':1,})#encoding
dataset_clean1['ThermalMassCategory']=dataset_clean1['ThermalMassCategory'].map({'Medium              ':0,
      'Medium-high         ':1,'Medium-low          ':2,
      'Low                 ':3,'High                ':4,})#encoding
dataset_clean1['PredominantRoofType']=dataset_clean1['PredominantRoofType'].map({
    'Pitch Roof-Insul.on Rafter    ':0,'Pitch Roof-Insul.on Ceiling   ':1,
    'Room in Roof-Insul.on side    ':2,'Flat Roof                     ':3,
    'Select Roof Type              ':4,})#encoding
dataset_clean1['PurposeOfRating']=dataset_clean1['PurposeOfRating'].map({
    'Sale':0,'New dwelling for owner occupat':1,'Unknown':2,'Social housing letting':3,
    'Private Letting':4,'Other':5,'Grant Support':6,})#encoding
dataset_clean1['FirstEnergyType_Description']=dataset_clean1['FirstEnergyType_Description'].map({
    'Not Renewable':0,'Renewable Electrical':1,'Renewable Thermal':2,})#encoding
dataset_clean1['SecondEnergyType_Description']=dataset_clean1['SecondEnergyType_Description'].map({
    'Not Renewable':0,'Renewable Electrical':1,'Renewable Thermal':2,})#encoding
dataset_clean1['ThirdEnergyType_Description']=dataset_clean1['ThirdEnergyType_Description'].map({
    'Not Renewable':0,'Renewable Electrical':1,'Renewable Thermal':2,})#encoding
dataset_clean1['FirstWallType_Description']=dataset_clean1['FirstWallType_Description'].map({
    '300mm Cavity':0,'Timber Frame':1,'300mm Filled Cavity':2,'Other':3,'Concrete Hollow Block':4,
    'Solid Mass Concrete':5,'225mm Solid brick':6,'Stone':7,'325mm Solid Brick':8,})#encoding

# Implementing AdaBoost
from sklearn.ensemble import AdaBoostClassifier
from sklearn.model_selection import GridSearchCV
from sklearn.pipeline import Pipeline
from sklearn.preprocessing import StandardScaler
from imblearn.over_sampling import SMOTE
from sklearn.impute import SimpleImputer

# Dividing dataset into label and feature sets
X = dataset_clean1.drop('EnergyRating', axis=1) # Features
Y = dataset_clean1['EnergyRating'] # Labels

# Impute missing values using Mean because the variance between the values are low.
imputer = SimpleImputer(strategy='mean')
X_imputed = imputer.fit_transform(X)

# Normalizing numerical features so that each feature has mean 0 and variance 1
feature_scaler = StandardScaler()
X_scaled = feature_scaler.fit_transform(X_imputed)

# Apply SMOTE for oversampling
smote = SMOTE(random_state=101)
X_resampled, Y_resampled = smote.fit_resample(X_scaled, Y)

# Implementing AdaBoost within a pipeline
model = Pipeline([
    ('classification', AdaBoostClassifier(random_state=1))
])

# Applying GridSearch
grid_param = {'classification__n_estimators': [10, 20, 30, 40, 50]}

gd_sr = GridSearchCV(estimator=model, param_grid=grid_param, scoring='recall', cv=3)
gd_sr.fit(X_resampled, Y_resampled)

best_parameters = gd_sr.best_params_
print(best_parameters)

best_result = gd_sr.best_score_ # Mean cross-validated score of the best_estimator
print(best_result)

featimp = pd.Series(gd_sr.best_estimator_.named_steps["classification"].feature_importances_, index=list(X)).sort_values(ascending=False)
print(featimp)

# Normalizing numerical features so that each feature has mean 0 and variance 1
# Splitting the dataset into training and testing sets
X_train, X_test, Y_train, Y_test = train_test_split( X_scaled, Y, test_size = 0.25, random_state = 100)
X_train,Y_train = SMOTE(random_state = 101).fit_resample(X_train,Y_train)

# Random Forest Classifier
from sklearn import ensemble
RF_classifier1 = ensemble.RandomForestClassifier(n_estimators=30, criterion='entropy', max_features='auto', random_state=1)  # building model
RF_classifier1.fit(X_train,Y_train)#training
Y_pred1 = RF_classifier1.predict(X_test) # testing
Important_features = Series(RF_classifier1.feature_importances_, index=list(X)).sort_values(ascending=False)
print(Important_features)

# Applying confusion matrix
from sklearn import metrics
Accuracy=metrics.accuracy_score(Y_test, Y_pred1) # calculating accuaracy
print("Accuracy: ", Accuracy) # Is this a good metric??
con_matrix = metrics.confusion_matrix(Y_test, Y_pred1)
print (con_matrix)

# Calculating recall and precision
recall = metrics.recall_score(Y_test, Y_pred1)
print (recall)
percision=metrics.precision_score(Y_test, Y_pred1)
print(percision)

# Implementing Logistic Regression
# Tuning eta0, max_iter, alpha, and l1_ratio parameters and implementing cross-validation using Grid Search
model = Pipeline([
        ('balancing', SMOTE(random_state = 101)),   # Synthetic Minority Oversampling Technique
        ('classification', SGDClassifier(loss = 'log_loss', penalty = 'elasticnet', random_state = 1))
    ])
grid_param = {'classification__eta0': [.01,.1,1,10], 'classification__max_iter' : [100], 'classification__alpha': [.01,.1, 1,10], 'classification__l1_ratio': [0,0.5,1]}

gd_sr = GridSearchCV(estimator=model, param_grid=grid_param, scoring='recall', cv=2)

gd_sr.fit(X_scaled, Y)

best_parameters = gd_sr.best_params_
print("Best parameters: ", best_parameters)

best_result = gd_sr.best_score_ # Mean cross-validated score of the best_estimator
print("Best result: ", best_result)